{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.util import trigrams\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import ssl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/lenguyen/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/lenguyen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/lenguyen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/lenguyen/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data\n",
    "\n",
    "Project Gutenberg: https://www.gutenberg.org/\n",
    "\n",
    "\n",
    "We load in the texts Alice in Wonderland, Moby Dick, and Romeo and Juliet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "aliceData = np.loadtxt('AliceInWonderland.txt', delimiter = \"\\n\",  dtype = str)\n",
    "mobyData = np.loadtxt('MobyDick.txt', delimiter = \"\\n\",  dtype = str)\n",
    "romeoData = np.loadtxt('RomeoAndJuliet.txt', delimiter = \"\\n\",  dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['large rabbit-hole under the hedge.',\n",
       "       'In another moment down went Alice after it, never once considering how',\n",
       "       'in the world she was to get out again.',\n",
       "       'The rabbit-hole went straight on like a tunnel for some way, and then',\n",
       "       'dipped suddenly down, so suddenly that Alice had not a moment to think',\n",
       "       'about stopping herself before she found herself falling down a very',\n",
       "       'deep well.',\n",
       "       'Either the well was very deep, or she fell very slowly, for she had',\n",
       "       'plenty of time as she went down to look about her and to wonder what',\n",
       "       'was going to happen next. First, she tried to look down and make out'],\n",
       "      dtype='<U82')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at our data\n",
    "aliceData[55:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "First we must tokenize the data, which means we need to take it out of long string sentance format into individual words which will be our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenAlice = [i.replace('.','').replace(',','').split(\" \") for i in aliceData]\n",
    "tokenMoby = [i.replace('.','').replace(',','').split(\" \") for i in mobyData]\n",
    "tokenRomeo = [i.replace('.','').replace(',','').split(\" \") for i in romeoData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['large', 'rabbit-hole', 'under', 'the', 'hedge']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenAlice[55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal\n",
    "\n",
    "Next, we remove articles in our sentences, such as \"is\", \"and\", \"the\" - they will not me helpful in our analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanAlice = []\n",
    "cleanMoby = []\n",
    "cleanRomeo = []\n",
    "\n",
    "for sentence in tokenAlice:\n",
    "    cleanAlice.append([i.lower() for i in sentence if i.lower() not in stopwords.words(\"english\")])\n",
    "    \n",
    "for sentence in tokenMoby:\n",
    "    cleanMoby.append([i.lower() for i in sentence if i.lower() not in stopwords.words(\"english\")])\n",
    "\n",
    "for sentence in tokenRomeo:\n",
    "    cleanRomeo.append([i.lower() for i in sentence if i.lower() not in stopwords.words(\"english\")])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['large', 'rabbit-hole', 'hedge'],\n",
       " ['another', 'moment', 'went', 'alice', 'never', 'considering'],\n",
       " ['world', 'get'],\n",
       " ['rabbit-hole', 'went', 'straight', 'like', 'tunnel', 'way'],\n",
       " ['dipped', 'suddenly', 'suddenly', 'alice', 'moment', 'think'],\n",
       " ['stopping', 'found', 'falling'],\n",
       " ['deep', 'well'],\n",
       " ['either', 'well', 'deep', 'fell', 'slowly'],\n",
       " ['plenty', 'time', 'went', 'look', 'wonder'],\n",
       " ['going', 'happen', 'next', 'first', 'tried', 'look', 'make']]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleanAlice[55:65]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization and Stemming\n",
    "\n",
    "Lemmatization and Stemming are both methods of removing prefixes and suffixes from words. Stemming is the most brute force and removes the prefix/suffix while Lemmatization replaces the word with the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:\n",
      "studies: studi\n",
      "\n",
      "Lemmatization\n",
      "studies: study\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"Stemming:\")\n",
    "print(\"studies:\", PorterStemmer().stem(\"studies\") + \"\\n\")\n",
    "print(\"Lemmatization\")\n",
    "print(\"studies:\", lemmatizer.lemmatize(\"studies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmaAlice = []\n",
    "lemmaMoby = []\n",
    "lemmaRomeo = []\n",
    "\n",
    "for sentence in cleanAlice:\n",
    "    lemmaAlice.append([lemmatizer.lemmatize(i) for i in sentence])\n",
    "    \n",
    "for sentence in cleanMoby:\n",
    "    lemmaMoby.append([lemmatizer.lemmatize(i) for i in sentence])\n",
    "\n",
    "for sentence in cleanRomeo:\n",
    "    lemmaRomeo.append([lemmatizer.lemmatize(i) for i in sentence])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordCount(words):\n",
    "    word_count = {}\n",
    "    for word in words:\n",
    "        if word not in word_count:\n",
    "            word_count[word] = 1\n",
    "        elif word in word_count:\n",
    "            word_count[word] += 1\n",
    "    word_count.pop('')\n",
    "    return sorted(word_count.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfAlice = getWordCount(np.concatenate(lemmaAlice))\n",
    "tfMoby = getWordCount(np.concatenate(lemmaMoby))\n",
    "tfRomeo = getWordCount(np.concatenate(lemmaRomeo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 450),\n",
       " ('alice', 351),\n",
       " ('little', 124),\n",
       " ('â€œi', 119),\n",
       " ('one', 87),\n",
       " ('work', 85),\n",
       " ('project', 83),\n",
       " ('went', 83),\n",
       " ('like', 77),\n",
       " ('could', 75)]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfAlice[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('whale', 1067),\n",
       " ('one', 883),\n",
       " ('like', 569),\n",
       " ('upon', 564),\n",
       " ('old', 438),\n",
       " ('would', 424),\n",
       " ('ship', 417),\n",
       " ('time', 412),\n",
       " ('sea', 397),\n",
       " ('ahab', 371)]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfMoby[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('romeo', 281),\n",
       " ('thou', 275),\n",
       " ('juliet', 175),\n",
       " ('thy', 170),\n",
       " ('capulet', 139),\n",
       " ('nurse', 134),\n",
       " ('thee', 130),\n",
       " ('love', 120),\n",
       " ('come', 120),\n",
       " ('shall', 112)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfRomeo[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "Look up all part of speech tags in NLTK: https://www.guru99.com/pos-tagging-chunking-nltk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobyTags = nltk.pos_tag([i[0] for i in tfMoby])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ship', 'VB'),\n",
       " ('little', 'VB'),\n",
       " ('say', 'VB'),\n",
       " ('u', 'VB'),\n",
       " ('thought', 'VB'),\n",
       " ('let', 'VB'),\n",
       " ('tell', 'VB'),\n",
       " ('mate', 'VB'),\n",
       " ('keep', 'VB'),\n",
       " ('talk', 'VB')]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sort by most used verb\n",
    "[i for i in mobyTags if i[-1] == \"VB\"][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
